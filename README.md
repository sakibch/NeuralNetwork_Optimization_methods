# Optimization_methods

This notebook explores different Optimization Methods for Neural Networks to accelerate convergence and improve optimization using Python.

The aims are to:
- Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
- Use random minibatches to accelerate convergence and improve optimization
